---
name: multi-agent-orchestrator
description: Orchestrate parallel CLI agents (Claude Code, Codex, Gemini) for competitive evaluation. Use when user says "run multi-agent", "compare agents", "launch competitive evaluation", "use parallel agents", or complex tasks (>7/10) where multiple approaches exist and best solution matters.
version: "2.0.0"
---
## ğŸ“‹ Multi-Agent Orchestrator = Competitive Parallel Execution

**Core Principle:** Launch N CLI agents (Claude Code, Codex, Gemini) with identical task â†’ Compare self-evaluations â†’ Declare winner based on measurable success criteria. **ğŸ”´ CRITICAL: NEVER MOCK DATA!** Try multiple approaches to get real data; if all fail, stop and document attempts.

**Multi-Agent Workflow Structure (Continuant - TD):**
```mermaid
graph TD
    Task[Task File] --> Claude[Claude Code Workspace]
    Task --> Codex[Codex CLI Workspace]
    Task --> Gemini[Gemini Workspace]

    Claude --> CPlan[01_plan_claude_code.md]
    Claude --> CResults[90_results_claude_code.md]
    Claude --> CArtifacts[All Artifacts]

    Codex --> DPlan[01_plan_codex.md]
    Codex --> DResults[90_results_codex.md]

    Gemini --> GPlan[01_plan_gemini.md]
    Gemini --> GResults[90_results_gemini.md]
```

**Orchestration Process Flow (Occurrent - LR):**
```mermaid
graph LR
    A[Agree on Folder] --> B[Create Draft Task]
    B --> C[User Edits File]
    C --> D[User Says Ready]
    D --> E[Launch Parallel Agents]
    E --> F[Monitor Plan Files]
    F --> G[Compare 90_results]
    G --> H[Declare Winner]
```
**Ontological Rule:** TD for workspace structure (what exists), LR for orchestration workflow (what happens)

**Primary source:** `algorithms/product_div/Multi_agent_framework/00_MULTI_AGENT_ORCHESTRATOR.md`
**Session ID:** `e9ce3592-bd66-4a98-b0e7-fcdd8edb5d42` by Daniel Kravtsov (2025-11-13) - v2.0.0
**Release log:** See `SKILL_RELEASE_LOG.md` for full version history

### ğŸ¯ When to Use

Â¶1 **Use multi-agent framework when:**
- Task complexity >7/10
- Multiple valid implementation approaches exist
- Need competitive evaluation
- Best solution critically important

Â¶2 **Use Task tool sub-agents when:**
- Single specialized capability (gmail, notion, jira)
- Standard workflow exists
- Quick operation needed
- Complexity <5/10

### ğŸ“ Setup Workflow

Â¶1 **MANDATORY FIRST STEP: Agree on Location**

Before creating anything, ask:
- "Where should I create this task folder?" (suggest 2-3 options based on task type)
- "What should the folder name be?" (format: `XX_descriptive_name`)

**Example:**
```
ğŸ¤–: "For your task, I suggest:
   1. /client_cases/[client]/15_[task]/ (if client-specific)
   2. /algorithms/product_div/15_[task]/ (if algorithm)

   Which location? And folder name?"

ğŸ‘¤: "Use client_cases/HP/15_customer_metrics/"

ğŸ¤–: "âœ… Creating task in: /client_cases/HP/15_customer_metrics/"
```

Â¶2 **Create Draft Task File Immediately**

After folder agreement, create quick draft - user will edit directly:

```bash
mkdir -p [agreed_path]
cd [agreed_path]

cat > 01_task_multi_agent.md << 'EOF'
## Task: [Your quick understanding]

**Success Criteria:** [DRAFT - user refines]
- [Draft criterion 1]
- [Draft criterion 2]

## Instructions for User:
1. ğŸ“ EDIT THIS FILE - Add details, fix criteria
2. âœ… CONFIRM - Reply "Ready" when good
3. ğŸ”„ ITERATE - Edit and reply with changes

**Current Status:** ğŸ”„ AWAITING YOUR EDITS

## Agents Artifact Requirement
Each agent MUST create:
- `01_plan_[agent].md` - Planning with progress updates
- `90_results_[agent].md` - Results with self-evaluation
- All outputs in workspace folder (claude_code/, codex_cli/, gemini/)

**Self-Evaluation Format:**
### Criterion 1: [from task]
**Status:** âœ…/âŒ/âš ï¸ | **Evidence:** [data] | **Details:** [how tested]

## Overall: X/Y criteria met | Grade: âœ…/âŒ/âš ï¸
EOF

mkdir -p claude_code codex_cli gemini
cd ..

echo "ğŸ“„ Task file: [agreed_path]/01_task_multi_agent.md"
echo "ğŸ”— file://[full_path]"
```

Â¶3 **User Edits Task File**

User has full control - edits file in IDE. No chat back-and-forth!

**User workflow:**
1. Open file (link provided)
2. Edit directly - improve description, refine criteria
3. Reply "Ready" or "Change criterion #2 to: [text]"

Â¶4 **Wait for Confirmation**

DO NOT PROCEED until user says "Ready".

**Acceptable:**
- âœ… "Ready"
- âœ… "Ready with changes: [edits]"
- âœ… "Change criterion #2 to: [text]"

### ğŸ”„ Execution

Â¶1 **Launch Parallel Agents**

When user says "Ready":

```bash
# Run in background
./run_parallel_agents.sh [agreed_path]/01_task_multi_agent.md &
SCRIPT_PID=$!

# Monitor progress
ps aux | grep $SCRIPT_PID
tail -f [task_folder]/*/claude_output.log
```

**Script location:** `algorithms/product_div/Multi_agent_framework/run_parallel_agents.sh`

**Scripts handle automatically:**
- âœ… Repository root execution
- âœ… .env file loading
- âœ… Workspace setup/cleanup
- âœ… Background process management
- âœ… Real-time monitoring (updates every 5s)

**Timing:**
- Codex: 2-3 min
- Claude: 5+ min
- Gemini: 3-5 min

Â¶2 **Monitor via Plan Files**

Track progress:
```bash
cat [agreed_path]/claude_code/01_*_plan_claude_code.md
cat [agreed_path]/codex_cli/01_*_plan_codex.md
cat [agreed_path]/gemini/01_*_plan_gemini.md
```

Â¶3 **Artifact Placement (CRITICAL)**

**ğŸ”´ ALL ARTIFACTS MUST BE IN AGENT WORKSPACE FOLDER**

Every agent MUST create ALL outputs in assigned workspace - NEVER in external directories.

**âŒ WRONG:**
```
[task]/
â”œâ”€â”€ claude_code/
â”‚   â”œâ”€â”€ 01_plan.md âœ…
â”‚   â””â”€â”€ 90_results.md âœ…
â”œâ”€â”€ data_processed/
â”‚   â””â”€â”€ output.csv âŒ WRONG!
â””â”€â”€ results.json âŒ WRONG!
```

**âœ… CORRECT:**
```
[task]/
â”œâ”€â”€ claude_code/
â”‚   â”œâ”€â”€ 01_plan.md âœ…
â”‚   â”œâ”€â”€ 90_results.md âœ…
â”‚   â”œâ”€â”€ output.csv âœ…
â”‚   â”œâ”€â”€ results.json âœ…
â”‚   â””â”€â”€ script.py âœ…
â””â”€â”€ codex_cli/
    â”œâ”€â”€ 01_plan.md âœ…
    â””â”€â”€ 90_results.md âœ…
```

**Why:**
1. Traceability - know which agent created what
2. Comparison - side-by-side outputs
3. Cleanup - delete failed results cleanly
4. Reproducibility - exact inputs/outputs

Â¶4 **Compare Self-Evaluations**

No manual testing - compare `90_results_*.md` files only:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Success Criteria  â”‚ Claude  â”‚ Codex â”‚ Gemini â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Process <5s       â”‚ âŒ 6.2s â”‚ âœ… 3.8â”‚ âœ… 4.1 â”‚
â”‚ Handle bad data   â”‚ âœ…      â”‚ âœ…    â”‚ âœ…     â”‚
â”‚ Unique approach   â”‚ âŒ      â”‚ âœ…    â”‚ âœ…     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CRITERIA MET      â”‚ 1/3     â”‚ 3/3   â”‚ 3/3    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ† WINNER: Tie Codex/Gemini
```

Winner = highest score (most âœ… criteria).

### ğŸ”— Scripts & References

Â¶1 **Ready-to-use scripts:**

**Main (recommended):**
```bash
./run_parallel_agents.sh task_file.md
```

**Individual:**
```bash
./run_claude_agent.sh task_file.md
./run_codex_agent.sh task_file.md
./run_gemini_agent.sh task_file.md
```

Â¶2 **Bundled resources:**

**Scripts:**
- `scripts/create_task_file.sh` - Generate standardized task files

**References:**
- `references/script_usage.md` - Detailed script documentation
- `references/task_templates.md` - Pre-built templates for common scenarios
- `algorithms/product_div/Multi_agent_framework/00_MULTI_AGENT_ORCHESTRATOR.md` - Full guide

**When to load:**
- Script errors â†’ Load `script_usage.md`
- Task templates â†’ Load `task_templates.md`
- Comprehensive understanding â†’ Load `00_MULTI_AGENT_ORCHESTRATOR.md`

### âŒ Anti-Patterns

Â¶1 **Common mistakes:**

âŒ Using for simple tasks (just do directly)
âŒ No clear success criteria (vague goals â†’ vague results)
âŒ Mocking data (NEVER create fake data)
âŒ Skipping user confirmation (always wait for "Ready")
âŒ External artifacts (all outputs in workspace folders)
âŒ Subjective evaluation (use measurable criteria only)

### âœ… Quick Reference

Â¶1 **Complete workflow:**

```
1. User describes complex task
2. Verify complexity >7/10
3. Agree on folder location
4. Create draft task file
5. User edits and confirms "Ready"
6. Launch ./run_parallel_agents.sh &
7. Monitor plan files
8. Compare 90_results_*.md
9. Declare winner by criteria met
10. Document results
```

Â¶2 **File templates:**

```markdown
# 01_plan_[agent].md
## My Approach ([agent])
- [ ] Step 1: [action]
## Progress: âœ… [timestamp] Step 1 complete

# 90_results_[agent].md
## Self-Evaluation ([agent])
### Criterion 1: [from task]
**Status:** âœ…/âŒ/âš ï¸ | **Evidence:** [data] | **Details:** [tested how]
## Overall: X/Y criteria | Grade: âœ…/âŒ/âš ï¸
```

Â¶3 **Folder structure:**

```
[agreed_path]/
â”œâ”€â”€ 01_task_multi_agent.md    # User-editable
â”œâ”€â”€ claude_code/              # Claude workspace
â”‚   â”œâ”€â”€ 01_*_plan_claude.md
â”‚   â””â”€â”€ 90_*_results_claude.md
â”œâ”€â”€ codex_cli/                # Codex workspace
â”‚   â”œâ”€â”€ 01_*_plan_codex.md
â”‚   â””â”€â”€ 90_*_results_codex.md
â””â”€â”€ gemini/                   # Gemini workspace
    â”œâ”€â”€ 01_*_plan_gemini.md
    â””â”€â”€ 90_*_results_gemini.md
```

---

**Meta Note:** See `knowledge-framework` skill for MECE/BFO principles. Multi-agent orchestrator uses CLI agents (not sub-agents), requires measurable success criteria, and selects winner through objective self-evaluation comparison.
